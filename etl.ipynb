{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b97d66a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ano mais recente: 2025\n",
      "URL selecionada: https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/2025/\n",
      "\n",
      "ZIPs encontrados:\n",
      " - 1T2025.zip\n",
      " - 2T2025.zip\n",
      " - 3T2025.zip\n",
      "\n",
      "Arquivo ZIP já existe: 1T2025.zip\n",
      "Conteúdo de 1T2025.zip já foi extraído anteriormente.\n",
      "\n",
      "Arquivo ZIP já existe: 2T2025.zip\n",
      "Conteúdo de 2T2025.zip já foi extraído anteriormente.\n",
      "\n",
      "Arquivo ZIP já existe: 3T2025.zip\n",
      "Conteúdo de 3T2025.zip já foi extraído anteriormente.\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "BASE_URL = \"https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "html = urlopen(BASE_URL).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "ano_atual = datetime.now().year\n",
    "anos = []\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href and href.endswith(\"/\") and href[:4].isdigit():\n",
    "        ano = int(href[:4])\n",
    "        if ano <= ano_atual:\n",
    "            anos.append(ano)\n",
    "\n",
    "ano_mais_recente = max(anos)\n",
    "URL_ANO = f\"{BASE_URL}{ano_mais_recente}/\"\n",
    "\n",
    "print(f\"Ano mais recente: {ano_mais_recente}\")\n",
    "print(f\"URL selecionada: {URL_ANO}\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "ANO_DIR = os.path.join(DATA_DIR, str(ano_mais_recente))\n",
    "os.makedirs(ANO_DIR, exist_ok=True)\n",
    "\n",
    "html_ano = urlopen(URL_ANO).read()\n",
    "soup_ano = BeautifulSoup(html_ano, \"html.parser\")\n",
    "\n",
    "zips = []\n",
    "for link in soup_ano.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href and href.lower().endswith(\".zip\"):\n",
    "        zips.append(href)\n",
    "\n",
    "print(\"\\nZIPs encontrados:\")\n",
    "for z in zips:\n",
    "    print(\" -\", z)\n",
    "\n",
    "for z in zips:\n",
    "    zip_url = URL_ANO + z\n",
    "    zip_path = os.path.join(ANO_DIR, z)\n",
    "    extract_folder = os.path.join(ANO_DIR, z.replace('.zip', ''))\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"\\nBaixando: {z}\")\n",
    "        urlretrieve(zip_url, zip_path)\n",
    "    else:\n",
    "        print(f\"\\nArquivo ZIP já existe: {z}\")\n",
    "\n",
    "    if not os.path.exists(extract_folder):\n",
    "        print(f\"Extraindo: {z} para {extract_folder}\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_folder)\n",
    "    else:\n",
    "        print(f\"Conteúdo de {z} já foi extraído anteriormente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff44cd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validando arquivos extraídos...\n",
      "Sucesso: 1T2025 contém 1 arquivo(s) CSV.\n",
      "Sucesso: 2T2025 contém 1 arquivo(s) CSV.\n",
      "Sucesso: 3T2025 contém 1 arquivo(s) CSV.\n"
     ]
    }
   ],
   "source": [
    "print(\"Validando arquivos extraídos...\")\n",
    "pastas_extracao = glob.glob(os.path.join(ANO_DIR, \"*/\"))\n",
    "\n",
    "for pasta in pastas_extracao:\n",
    "    csvs = glob.glob(os.path.join(pasta, \"*.csv\"))\n",
    "    if len(csvs) > 0:\n",
    "        print(f\"Sucesso: {os.path.basename(os.path.normpath(pasta))} contém {len(csvs)} arquivo(s) CSV.\")\n",
    "    else:\n",
    "        print(f\"Erro: A pasta {pasta} está vazia ou não contém CSVs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efaeefb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando encoding dos arquivos...\n",
      "✅ 1T2025.csv: UTF-8\n",
      "✅ 2T2025.csv: UTF-8\n",
      "✅ 3T2025.csv: UTF-8\n"
     ]
    }
   ],
   "source": [
    "print(\"Verificando encoding dos arquivos...\")\n",
    "for pasta in pastas_extracao:\n",
    "    csv_file = glob.glob(os.path.join(pasta, \"*.csv\"))[0]\n",
    "    try:\n",
    "        \n",
    "        pd.read_csv(csv_file, sep=None, engine='python', nrows=5, encoding='utf-8')\n",
    "        print(f\"✅ {os.path.basename(csv_file)}: UTF-8\")\n",
    "    except:\n",
    "        try:\n",
    "        \n",
    "            pd.read_csv(csv_file, sep=None, engine='python', nrows=5, encoding='latin-1')\n",
    "            print(f\"{os.path.basename(csv_file)}: ISO-8859-1 (Latin-1)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao detectar encoding de {csv_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d52f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incluido: 1T2025.csv\n",
      "Incluido: 2T2025.csv\n",
      "Incluido: 3T2025.csv\n",
      "Linhas no merge: 2113924\n"
     ]
    }
   ],
   "source": [
    "lista_para_merge = []\n",
    "termo_gatilho = \"Despesas com Eventos/Sinistros\"\n",
    "\n",
    "caminhos_csv = glob.glob(os.path.join(ANO_DIR, \"**/*.csv\"), recursive=True)\n",
    "\n",
    "for caminho in caminhos_csv:\n",
    "    if \"output\" in caminho:\n",
    "        continue\n",
    "        \n",
    "    nome = os.path.basename(caminho)\n",
    "    df_individual = pd.read_csv(caminho, sep=';', encoding='latin-1', low_memory=False)\n",
    "    \n",
    "    if df_individual['DESCRICAO'].astype(str).str.contains(termo_gatilho, case=False).any():\n",
    "        print(f\"Incluido: {nome}\")\n",
    "        lista_para_merge.append(df_individual)\n",
    "    else:\n",
    "        print(f\"Ignorado: {nome}\")\n",
    "\n",
    "if lista_para_merge:\n",
    "    df_consolidado = pd.concat(lista_para_merge, ignore_index=True)\n",
    "    print(f\"Linhas no merge: {len(df_consolidado)}\")\n",
    "else:\n",
    "    df_consolidado = pd.DataFrame()\n",
    "    print(\"Nenhum arquivo validado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c893f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos ja existem. Pulando processamento.\n"
     ]
    }
   ],
   "source": [
    "BASE_OUTPUT = \"output\"\n",
    "SUBPASTA_CONSOLIDADO = os.path.join(BASE_OUTPUT, \"massa_dados_trimestrais\")\n",
    "os.makedirs(SUBPASTA_CONSOLIDADO, exist_ok=True)\n",
    "\n",
    "final_csv = os.path.join(SUBPASTA_CONSOLIDADO, \"dados_contabeis_unificados.csv\")\n",
    "final_zip = os.path.join(SUBPASTA_CONSOLIDADO, \"dados_contabeis_unificados.zip\")\n",
    "\n",
    "if os.path.exists(final_csv) and os.path.exists(final_zip):\n",
    "    print(\"Arquivos ja existem. Pulando processamento.\")\n",
    "else:\n",
    "    if not df_consolidado.empty:\n",
    "\n",
    "        df_consolidado['CD_CONTA_CONTABIL'] = df_consolidado['CD_CONTA_CONTABIL'].astype(str)\n",
    "        \n",
    "\n",
    "        df_consolidado = df_consolidado[df_consolidado['CD_CONTA_CONTABIL'].str.startswith('4')].copy()\n",
    "\n",
    "        df_consolidado['DATA'] = pd.to_datetime(df_consolidado['DATA'], errors='coerce')\n",
    "        df_consolidado['REG_ANS'] = df_consolidado['REG_ANS'].astype(str).str.strip()\n",
    "        \n",
    "        for col in ['VL_SALDO_INICIAL', 'VL_SALDO_FINAL']:\n",
    "            if df_consolidado[col].dtype == 'object':\n",
    "                df_consolidado[col] = df_consolidado[col].astype(str).str.replace(',', '.')\n",
    "            df_consolidado[col] = pd.to_numeric(df_consolidado[col], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "        df_consolidado['DESCRICAO'] = df_consolidado['DESCRICAO'].fillna('NAO INFORMADO').str.strip().str.upper()\n",
    "        \n",
    "        df_consolidado.to_csv(final_csv, sep=';', index=False, encoding='utf-8')\n",
    "        df_consolidado.to_csv(final_zip, sep=';', index=False, encoding='utf-8', compression={'method': 'zip', 'archive_name': 'dados_contabeis_unificados.csv'})\n",
    "        print(f\"Arquivos gerados em: {SUBPASTA_CONSOLIDADO}\")\n",
    "    else:\n",
    "        print(\"DataFrame vazio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5c7500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Relatorio_cadop.csv ja existe.\n"
     ]
    }
   ],
   "source": [
    "SUBPASTA_CADASTRO = os.path.join(\"output\", \"dados_cadastrais\")\n",
    "os.makedirs(SUBPASTA_CADASTRO, exist_ok=True)\n",
    "\n",
    "BASE_URL_CADASTRO = \"https://dadosabertos.ans.gov.br/FTP/PDA/operadoras_de_plano_de_saude_ativas/\"\n",
    "html_cad = urlopen(BASE_URL_CADASTRO).read()\n",
    "soup_cad = BeautifulSoup(html_cad, \"html.parser\")\n",
    "\n",
    "arquivo_csv = \"\"\n",
    "for link in soup_cad.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href and href.endswith(\".csv\"):\n",
    "        arquivo_csv = href\n",
    "        break\n",
    "\n",
    "if arquivo_csv:\n",
    "    url_final_cadastro = BASE_URL_CADASTRO + arquivo_csv\n",
    "    caminho_destino = os.path.join(SUBPASTA_CADASTRO, arquivo_csv)\n",
    "    \n",
    "    if not os.path.exists(caminho_destino):\n",
    "        print(f\"Baixando: {arquivo_csv}\")\n",
    "        urlretrieve(url_final_cadastro, caminho_destino)\n",
    "    else:\n",
    "        print(f\"Arquivo {arquivo_csv} ja existe.\")\n",
    "    \n",
    "    df_cadastro = pd.read_csv(caminho_destino, sep=';', encoding='latin-1', dtype={'CNPJ': str, 'Registro_ANS': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "384f66f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Inconsistências (Tarefa 1.3):\n",
      "- Datas fora do padrão YYYY-MM-DD: 0\n",
      "- Valores estritamente negativos: 163802\n",
      "- Valores zerados: 688365\n",
      "- Valores não numéricos (NaN): 0\n",
      "- Total de valores suspeitos: 852167\n"
     ]
    }
   ],
   "source": [
    "df_contabil_validado = df_consolidado.copy()\n",
    "\n",
    "df_contabil_validado['VL_SALDO_FINAL'] = pd.to_numeric(\n",
    "    df_contabil_validado['VL_SALDO_FINAL'].astype(str).str.replace(',', '.'), \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "df_contabil_validado['valor_negativo'] = (df_contabil_validado['VL_SALDO_FINAL'] < 0).astype(int)\n",
    "df_contabil_validado['valor_zero'] = (df_contabil_validado['VL_SALDO_FINAL'] == 0).astype(int)\n",
    "df_contabil_validado['valor_nulo'] = df_contabil_validado['VL_SALDO_FINAL'].isna().astype(int)\n",
    "\n",
    "df_contabil_validado['valor_suspeito'] = np.where(\n",
    "    df_contabil_validado['valor_nulo'] | df_contabil_validado['valor_negativo'] | df_contabil_validado['valor_zero'], \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "\n",
    "df_contabil_validado['DATA_CONVERTIDA'] = pd.to_datetime(\n",
    "    df_contabil_validado['DATA'], \n",
    "    format='%Y-%m-%d', \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "df_contabil_validado['data_inconsistente'] = df_contabil_validado['DATA_CONVERTIDA'].isna().astype(int)\n",
    "df_contabil_validado['Ano'] = df_contabil_validado['DATA_CONVERTIDA'].dt.year\n",
    "df_contabil_validado['Trimestre'] = df_contabil_validado['DATA_CONVERTIDA'].dt.quarter\n",
    "\n",
    "print(f\"Relatório de Inconsistências (Tarefa 1.3):\")\n",
    "print(f\"- Datas fora do padrão YYYY-MM-DD: {df_contabil_validado['data_inconsistente'].sum()}\")\n",
    "print(f\"- Valores estritamente negativos: {df_contabil_validado['valor_negativo'].sum()}\")\n",
    "print(f\"- Valores zerados: {df_contabil_validado['valor_zero'].sum()}\")\n",
    "print(f\"- Valores não numéricos (NaN): {df_contabil_validado['valor_nulo'].sum()}\")\n",
    "print(f\"- Total de valores suspeitos: {df_contabil_validado['valor_suspeito'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75679cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhuma inconsistencia de CNPJ encontrada.\n"
     ]
    }
   ],
   "source": [
    "df_cadastro['casos_suspeitos'] = df_cadastro.groupby('CNPJ')['Razao_Social'].transform('nunique')\n",
    "df_cadastro['casos_suspeitos'] = (df_cadastro['casos_suspeitos'] > 1).astype(int)\n",
    "\n",
    "df_conflitos = df_cadastro[df_cadastro['casos_suspeitos'] == 1].groupby('CNPJ')['Razao_Social'].unique().reset_index()\n",
    "\n",
    "if not df_conflitos.empty:\n",
    "    print(f\"Total de CNPJs com duplicidade de Razao Social: {len(df_conflitos)}\")\n",
    "    print(\"\\nExemplos de conflitos (Top 10):\")\n",
    "    print(df_conflitos.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"Nenhuma inconsistencia de CNPJ encontrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09443aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos já existem em output/consolidado. Pulando processamento.\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'output/consolidado'\n",
    "csv_path = os.path.join(output_dir, 'consolidado_despesas.csv')\n",
    "zip_path = os.path.join(output_dir, 'consolidado_despesas.zip')\n",
    "\n",
    "if os.path.exists(csv_path) and os.path.exists(zip_path):\n",
    "    print(f\"Arquivos já existem em {output_dir}. Pulando processamento.\")\n",
    "    df_final = pd.read_csv(csv_path, sep=';')\n",
    "else:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    col_registro_cad = 'REGISTRO_OPERADORA'\n",
    "    col_registro_cont = 'REG_ANS'\n",
    "\n",
    "    df_contabil_validado[col_registro_cont] = df_contabil_validado[col_registro_cont].astype(str).str.strip()\n",
    "    df_cadastro[col_registro_cad] = df_cadastro[col_registro_cad].astype(str).str.strip()\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_contabil_validado,\n",
    "        df_cadastro[[col_registro_cad, 'CNPJ', 'Razao_Social', 'casos_suspeitos', 'Modalidade', 'UF']],\n",
    "        left_on=col_registro_cont,\n",
    "        right_on=col_registro_cad,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    df_entrega = df_final[[\n",
    "        'CNPJ', 'Razao_Social', 'Trimestre', 'Ano', 'VL_SALDO_FINAL',\n",
    "        'data_inconsistente', 'valor_suspeito', 'casos_suspeitos'\n",
    "    ]].rename(columns={'Razao_Social': 'RazaoSocial', 'VL_SALDO_FINAL': 'ValorDespesas'})\n",
    "\n",
    "    df_entrega.to_csv(csv_path, sep=';', index=False, encoding='utf-8')\n",
    "    df_entrega.to_csv(\n",
    "        zip_path, \n",
    "        sep=';', \n",
    "        index=False, \n",
    "        encoding='utf-8', \n",
    "        compression={'method': 'zip', 'archive_name': 'consolidado_despesas.csv'}\n",
    "    )\n",
    "    print(\"Processamento concluído e arquivos salvos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c1251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo output/agregado\\despesas_agregadas.csv já existe. Pulando agregação.\n"
     ]
    }
   ],
   "source": [
    "def validar_cnpj(cnpj):\n",
    "    cnpj = re.sub(r'\\D', '', str(cnpj))\n",
    "    if len(cnpj) != 14 or len(set(cnpj)) == 1:\n",
    "        return False\n",
    "    def calcular_digito(cnpj, pesos):\n",
    "        soma = sum(int(a) * b for a, b in zip(cnpj, pesos))\n",
    "        resto = soma % 11\n",
    "        return 0 if resto < 2 else 11 - resto\n",
    "    pesos1 = [5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "    pesos2 = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "    return int(cnpj[12]) == calcular_digito(cnpj[:12], pesos1) and \\\n",
    "           int(cnpj[13]) == calcular_digito(cnpj[:13], pesos2)\n",
    "\n",
    "output_dir_agregado = 'output/agregado'\n",
    "agregado_path = os.path.join(output_dir_agregado, 'despesas_agregadas.csv')\n",
    "zip_final_path = os.path.join(output_dir_agregado, 'Teste_Nathan.zip')\n",
    "\n",
    "if os.path.exists(agregado_path):\n",
    "    print(f\"Arquivo {agregado_path} já existe. Pulando agregação.\")\n",
    "else:\n",
    "    os.makedirs(output_dir_agregado, exist_ok=True)\n",
    "    \n",
    "    col_reg = 'REGISTRO_OPERADORA'\n",
    "    df_validacao = pd.merge(\n",
    "        df_contabil_validado,\n",
    "        df_cadastro[[col_reg, 'CNPJ', 'Razao_Social', 'Modalidade', 'UF']],\n",
    "        left_on='REG_ANS',\n",
    "        right_on=col_reg,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    df_limpo = df_validacao[\n",
    "        (df_validacao['VL_SALDO_FINAL'] > 0) & \n",
    "        (df_validacao['Razao_Social'].notna()) & \n",
    "        (df_validacao['Razao_Social'].str.strip() != '')\n",
    "    ].copy()\n",
    "\n",
    "    df_limpo['cnpj_valido'] = df_limpo['CNPJ'].apply(validar_cnpj)\n",
    "    df_limpo = df_limpo[df_limpo['cnpj_valido'] == True]\n",
    "\n",
    "    df_agregado = df_limpo.groupby(\n",
    "        ['CNPJ', 'Razao_Social', 'Modalidade', 'UF', 'Trimestre', 'Ano', col_reg]\n",
    "    ).agg(\n",
    "        ValorDespesas=('VL_SALDO_FINAL', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    stats = df_agregado.groupby(['CNPJ', 'Razao_Social']).agg(\n",
    "        MediaTrimestral=('ValorDespesas', 'mean'),\n",
    "        DesvioPadraoTrimestral=('ValorDespesas', 'std')\n",
    "    ).reset_index()\n",
    "\n",
    "    df_final_entrega = pd.merge(df_agregado, stats, on=['CNPJ', 'Razao_Social'], how='left')\n",
    "\n",
    "    df_final_entrega = df_final_entrega.rename(columns={\n",
    "        'Razao_Social': 'RazaoSocial',\n",
    "        col_reg: 'RegistroANS'\n",
    "    })\n",
    "\n",
    "    df_final_entrega = df_final_entrega.sort_values(by=['ValorDespesas'], ascending=False)\n",
    "\n",
    "    df_final_entrega.to_csv(agregado_path, sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    df_final_entrega.to_csv(\n",
    "        zip_final_path, \n",
    "        sep=';', \n",
    "        index=False, \n",
    "        encoding='utf-8', \n",
    "        compression={'method': 'zip', 'archive_name': 'despesas_agregadas.csv'}\n",
    "    )\n",
    "    print(\"Processamento concluído com métricas trimestrais.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
