{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b97d66a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ano mais recente: 2025\n",
      "URL selecionada: https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/2025/\n",
      "\n",
      "ZIPs encontrados:\n",
      " - 1T2025.zip\n",
      " - 2T2025.zip\n",
      " - 3T2025.zip\n",
      "\n",
      "Arquivo ZIP já existe: 1T2025.zip\n",
      "Conteúdo de 1T2025.zip já foi extraído anteriormente.\n",
      "\n",
      "Arquivo ZIP já existe: 2T2025.zip\n",
      "Conteúdo de 2T2025.zip já foi extraído anteriormente.\n",
      "\n",
      "Arquivo ZIP já existe: 3T2025.zip\n",
      "Conteúdo de 3T2025.zip já foi extraído anteriormente.\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "BASE_URL = \"https://dadosabertos.ans.gov.br/FTP/PDA/demonstracoes_contabeis/\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "html = urlopen(BASE_URL).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "ano_atual = datetime.now().year\n",
    "anos = []\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href and href.endswith(\"/\") and href[:4].isdigit():\n",
    "        ano = int(href[:4])\n",
    "        if ano <= ano_atual:\n",
    "            anos.append(ano)\n",
    "\n",
    "ano_mais_recente = max(anos)\n",
    "URL_ANO = f\"{BASE_URL}{ano_mais_recente}/\"\n",
    "\n",
    "print(f\"Ano mais recente: {ano_mais_recente}\")\n",
    "print(f\"URL selecionada: {URL_ANO}\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "ANO_DIR = os.path.join(DATA_DIR, str(ano_mais_recente))\n",
    "os.makedirs(ANO_DIR, exist_ok=True)\n",
    "\n",
    "html_ano = urlopen(URL_ANO).read()\n",
    "soup_ano = BeautifulSoup(html_ano, \"html.parser\")\n",
    "\n",
    "zips = []\n",
    "for link in soup_ano.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href and href.lower().endswith(\".zip\"):\n",
    "        zips.append(href)\n",
    "\n",
    "print(\"\\nZIPs encontrados:\")\n",
    "for z in zips:\n",
    "    print(\" -\", z)\n",
    "\n",
    "for z in zips:\n",
    "    zip_url = URL_ANO + z\n",
    "    zip_path = os.path.join(ANO_DIR, z)\n",
    "    extract_folder = os.path.join(ANO_DIR, z.replace('.zip', ''))\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"\\nBaixando: {z}\")\n",
    "        urlretrieve(zip_url, zip_path)\n",
    "    else:\n",
    "        print(f\"\\nArquivo ZIP já existe: {z}\")\n",
    "\n",
    "    if not os.path.exists(extract_folder):\n",
    "        print(f\"Extraindo: {z} para {extract_folder}\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_folder)\n",
    "    else:\n",
    "        print(f\"Conteúdo de {z} já foi extraído anteriormente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff44cd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validando arquivos extraídos...\n",
      "Sucesso: 1T2025 contém 1 arquivo(s) CSV.\n",
      "Sucesso: 2T2025 contém 1 arquivo(s) CSV.\n",
      "Sucesso: 3T2025 contém 1 arquivo(s) CSV.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "print(\"Validando arquivos extraídos...\")\n",
    "pastas_extracao = glob.glob(os.path.join(ANO_DIR, \"*/\"))\n",
    "\n",
    "for pasta in pastas_extracao:\n",
    "    csvs = glob.glob(os.path.join(pasta, \"*.csv\"))\n",
    "    if len(csvs) > 0:\n",
    "        print(f\"Sucesso: {os.path.basename(os.path.normpath(pasta))} contém {len(csvs)} arquivo(s) CSV.\")\n",
    "    else:\n",
    "        print(f\"Erro: A pasta {pasta} está vazia ou não contém CSVs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efaeefb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando encoding dos arquivos...\n",
      "✅ 1T2025.csv: UTF-8\n",
      "✅ 2T2025.csv: UTF-8\n",
      "✅ 3T2025.csv: UTF-8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Verificando encoding dos arquivos...\")\n",
    "for pasta in pastas_extracao:\n",
    "    csv_file = glob.glob(os.path.join(pasta, \"*.csv\"))[0]\n",
    "    try:\n",
    "        \n",
    "        pd.read_csv(csv_file, sep=None, engine='python', nrows=5, encoding='utf-8')\n",
    "        print(f\"✅ {os.path.basename(csv_file)}: UTF-8\")\n",
    "    except:\n",
    "        try:\n",
    "        \n",
    "            pd.read_csv(csv_file, sep=None, engine='python', nrows=5, encoding='latin-1')\n",
    "            print(f\"{os.path.basename(csv_file)}: ISO-8859-1 (Latin-1)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao detectar encoding de {csv_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d52f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incluido: 1T2025.csv\n",
      "Incluido: 2T2025.csv\n",
      "Incluido: 3T2025.csv\n",
      "Linhas no merge: 2113924\n"
     ]
    }
   ],
   "source": [
    "lista_para_merge = []\n",
    "termo_gatilho = \"EVENTOS/ SINISTROS CONHECIDOS OU AVISADOS\"\n",
    "\n",
    "caminhos_csv = glob.glob(os.path.join(ANO_DIR, \"**/*.csv\"), recursive=True)\n",
    "\n",
    "for caminho in caminhos_csv:\n",
    "    if \"output\" in caminho:\n",
    "        continue\n",
    "        \n",
    "    nome = os.path.basename(caminho)\n",
    "    df_individual = pd.read_csv(caminho, sep=';', encoding='latin-1', low_memory=False)\n",
    "    \n",
    "    if df_individual['DESCRICAO'].astype(str).str.contains(termo_gatilho, case=False).any():\n",
    "        print(f\"Incluido: {nome}\")\n",
    "        lista_para_merge.append(df_individual)\n",
    "    else:\n",
    "        print(f\"Ignorado: {nome}\")\n",
    "\n",
    "if lista_para_merge:\n",
    "    df_consolidado = pd.concat(lista_para_merge, ignore_index=True)\n",
    "    print(f\"Linhas no merge: {len(df_consolidado)}\")\n",
    "else:\n",
    "    df_consolidado = pd.DataFrame()\n",
    "    print(\"Nenhum arquivo validado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c893f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos ja existem. Pulando processamento.\n"
     ]
    }
   ],
   "source": [
    "BASE_OUTPUT = \"output\"\n",
    "SUBPASTA_CONSOLIDADO = os.path.join(BASE_OUTPUT, \"massa_dados_trimestrais\")\n",
    "os.makedirs(SUBPASTA_CONSOLIDADO, exist_ok=True)\n",
    "\n",
    "final_csv = os.path.join(SUBPASTA_CONSOLIDADO, \"dados_contabeis_unificados.csv\")\n",
    "final_zip = os.path.join(SUBPASTA_CONSOLIDADO, \"dados_contabeis_unificados.zip\")\n",
    "\n",
    "if os.path.exists(final_csv) and os.path.exists(final_zip):\n",
    "    print(\"Arquivos ja existem. Pulando processamento.\")\n",
    "else:\n",
    "    if not df_consolidado.empty:\n",
    "        df_consolidado['DATA'] = pd.to_datetime(df_consolidado['DATA'], errors='coerce')\n",
    "        df_consolidado['REG_ANS'] = df_consolidado['REG_ANS'].astype(str)\n",
    "        df_consolidado['CD_CONTA_CONTABIL'] = df_consolidado['CD_CONTA_CONTABIL'].astype(str)\n",
    "        \n",
    "        for col in ['VL_SALDO_INICIAL', 'VL_SALDO_FINAL']:\n",
    "            if df_consolidado[col].dtype == 'object':\n",
    "                df_consolidado[col] = df_consolidado[col].str.replace(',', '.').astype(float)\n",
    "            df_consolidado[col] = df_consolidado[col].fillna(0)\n",
    "\n",
    "        df_consolidado['DESCRICAO'] = df_consolidado['DESCRICAO'].fillna('NAO INFORMADO').str.strip().str.upper()\n",
    "\n",
    "        df_consolidado.to_csv(final_csv, sep=';', index=False, encoding='utf-8')\n",
    "        df_consolidado.to_csv(final_zip, sep=';', index=False, encoding='utf-8', compression={'method': 'zip', 'archive_name': 'dados_contabeis_unificados.csv'})\n",
    "        print(f\"Arquivos gerados em: {SUBPASTA_CONSOLIDADO}\")\n",
    "    else:\n",
    "        print(\"DataFrame vazio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5c7500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Relatorio_cadop.csv ja existe.\n"
     ]
    }
   ],
   "source": [
    "SUBPASTA_CADASTRO = os.path.join(\"output\", \"dados_cadastrais\")\n",
    "os.makedirs(SUBPASTA_CADASTRO, exist_ok=True)\n",
    "\n",
    "BASE_URL_CADASTRO = \"https://dadosabertos.ans.gov.br/FTP/PDA/operadoras_de_plano_de_saude_ativas/\"\n",
    "html_cad = urlopen(BASE_URL_CADASTRO).read()\n",
    "soup_cad = BeautifulSoup(html_cad, \"html.parser\")\n",
    "\n",
    "arquivo_csv = \"\"\n",
    "for link in soup_cad.find_all(\"a\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href and href.endswith(\".csv\"):\n",
    "        arquivo_csv = href\n",
    "        break\n",
    "\n",
    "if arquivo_csv:\n",
    "    url_final_cadastro = BASE_URL_CADASTRO + arquivo_csv\n",
    "    caminho_destino = os.path.join(SUBPASTA_CADASTRO, arquivo_csv)\n",
    "    \n",
    "    if not os.path.exists(caminho_destino):\n",
    "        print(f\"Baixando: {arquivo_csv}\")\n",
    "        urlretrieve(url_final_cadastro, caminho_destino)\n",
    "    else:\n",
    "        print(f\"Arquivo {arquivo_csv} ja existe.\")\n",
    "    \n",
    "    df_cadastro = pd.read_csv(caminho_destino, sep=';', encoding='latin-1', dtype={'CNPJ': str, 'Registro_ANS': str})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
